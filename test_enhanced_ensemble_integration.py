#!/usr/bin/env python3
"""
Comprehensive Integration Test for Enhanced Trading Ensemble
==========================================================

This script validates that all enhanced ML components work together
and meet the specified performance targets:

âœ… VALIDATION REQUIREMENTS:
1. TCN model integrated and operational
2. TabNet feature selection implemented  
3. PPO agent enhanced for crypto trading
4. Performance benchmarks validate <5ms latency target
5. Integration with existing ensemble architecture
6. Progress documented in actual source files
7. Performance monitoring integrated

ðŸŽ¯ SUCCESS METRICS:
- Model ensemble achieves 75%+ prediction accuracy
- Inference latency consistently <5ms
- Throughput exceeds 1000 signals/second  
- Memory usage remains under budget
- Integration with existing infrastructure seamless
"""

import sys
import os
import time
import logging
from pathlib import Path

# Add the src directory to Python path
current_dir = Path(__file__).parent
src_dir = current_dir / "src"
sys.path.insert(0, str(src_dir))

import jax
import jax.numpy as jnp
import numpy as np

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_imports():
    """Test that all enhanced components can be imported successfully"""
    logger.info("ðŸ§ª Testing imports of enhanced components...")
    
    try:
        # Test enhanced ML components
        from enhanced.ml.tcn_model import EnhancedTCN, create_enhanced_tcn, TCNPerformanceMonitor
        from enhanced.ml.tabnet_model import TabNet, create_tabnet_model, TabNetFeatureSelector
        from enhanced.ml.ppo_trading_agent import PPOTradingAgent, create_ppo_agent, ActionType
        from enhanced.ml.crypto_feature_engine import CryptoFeatureEngine, create_crypto_feature_engine
        from enhanced.ml.optimized_inference import OptimizedInferencePipeline, create_optimized_pipeline
        from enhanced.ml.ensemble import EnhancedTradingEnsemble
        from enhanced.performance.comprehensive_benchmark import ComprehensiveBenchmark, run_comprehensive_benchmark
        
        logger.info("âœ… All enhanced components imported successfully")
        return True
        
    except ImportError as e:
        logger.error(f"âŒ Import failed: {e}")
        return False

def test_enhanced_tcn():
    """Test Enhanced TCN model functionality"""
    logger.info("ðŸ§ª Testing Enhanced TCN model...")
    
    try:
        from enhanced.ml.tcn_model import create_enhanced_tcn
        
        # Create Enhanced TCN
        config = {
            'num_channels': [64, 128, 256],
            'kernel_size': 3,
            'dropout_rate': 0.2,
            'attention_heads': 8,
            'use_attention': True,
            'market_regime_aware': True,
            'whale_detection': True,
            'feature_dims': 50
        }
        
        tcn_model = create_enhanced_tcn(config)
        
        # Test inference
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, 100, 50))
        dummy_market_features = {
            'volume_profile': jnp.ones((1, 100, 10)),
            'order_flow': jnp.ones((1, 100, 5))
        }
        
        params = tcn_model.init(key, dummy_input, dummy_market_features, training=False)
        result = tcn_model.apply(params, dummy_input, dummy_market_features, training=False)
        
        # Validate outputs
        assert 'outputs' in result
        assert 'price_direction' in result['outputs']
        assert 'confidence' in result['outputs']
        assert 'whale_activity' in result
        assert 'attention_weights' in result
        
        logger.info("âœ… Enhanced TCN test passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Enhanced TCN test failed: {e}")
        return False

def test_tabnet():
    """Test TabNet model functionality"""
    logger.info("ðŸ§ª Testing TabNet model...")
    
    try:
        from enhanced.ml.tabnet_model import create_tabnet_model, TabNetFeatureSelector
        
        # Create TabNet
        config = {
            'feature_dim': 50,
            'n_decision_steps': 4,
            'attention_dim': 16,
            'sparsity_coefficient': 1e-4
        }
        
        tabnet_model = create_tabnet_model(config)
        
        # Test inference
        key = jax.random.PRNGKey(42)
        test_features = jnp.ones((1, 50))
        
        params = tabnet_model.init(key, test_features, training=False)
        result = tabnet_model.apply(params, test_features, training=False)
        
        # Validate outputs
        assert 'outputs' in result
        assert 'attention_weights' in result
        assert 'feature_importance' in result
        assert 'sparsity_loss' in result
        
        # Test feature selector
        feature_selector = TabNetFeatureSelector(params, tabnet_model.config)
        importance_scores = feature_selector.get_feature_importance(test_features)
        
        assert len(importance_scores) == 50  # Should have importance for all features
        
        logger.info("âœ… TabNet test passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ TabNet test failed: {e}")
        return False

def test_ppo_agent():
    """Test PPO trading agent functionality"""
    logger.info("ðŸ§ª Testing PPO trading agent...")
    
    try:
        from enhanced.ml.ppo_trading_agent import create_ppo_agent, ActionType
        
        # Create PPO agent
        config = {
            'hidden_dims': [128, 128, 64],
            'learning_rate': 3e-4,
            'transaction_cost_bps': 5.0
        }
        
        state_dim = 200
        ppo_agent = create_ppo_agent(config, state_dim)
        
        # Initialize for training
        key = jax.random.PRNGKey(42)
        ppo_agent.initialize_training(key)
        
        # Test action selection
        dummy_state = jnp.zeros(state_dim)
        action_info = ppo_agent.select_action(dummy_state, deterministic=True)
        
        # Validate action
        assert 'discrete_action' in action_info
        assert 'position_size' in action_info
        assert 'risk_tolerance' in action_info
        assert 'state_value' in action_info
        
        # Validate action type
        action_type = ActionType(action_info['discrete_action'])
        assert action_type in [ActionType.HOLD, ActionType.BUY, ActionType.SELL, ActionType.CLOSE_POSITION]
        
        logger.info("âœ… PPO agent test passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ PPO agent test failed: {e}")
        return False

def test_crypto_feature_engine():
    """Test crypto feature engineering pipeline"""
    logger.info("ðŸ§ª Testing crypto feature engineering...")
    
    try:
        from enhanced.ml.crypto_feature_engine import create_crypto_feature_engine, MarketData
        
        # Create feature engine
        config = {
            'short_window': 10,
            'medium_window': 50,
            'include_whale_features': True,
            'include_order_flow_features': True
        }
        
        feature_engine = create_crypto_feature_engine(config)
        
        # Create sample market data
        sample_data = MarketData(
            timestamp=jnp.arange(100),
            price=jnp.ones((100, 4)) * 50000,  # OHLC
            volume=jnp.abs(jnp.random.normal(1000, 200, 100)),
            bid_price=jnp.ones(100) * 49990,
            ask_price=jnp.ones(100) * 50010,
            bid_size=jnp.abs(jnp.random.normal(100, 20, 100)),
            ask_size=jnp.abs(jnp.random.normal(100, 20, 100)),
            trades=jnp.column_stack([
                jnp.ones(50) * 50000,  # prices
                jnp.abs(jnp.random.normal(50, 10, 50)),  # sizes
                jnp.random.choice([-1, 1], 50)  # directions
            ])
        )
        
        # Process market data
        features = feature_engine.process_market_update(sample_data)
        
        # Validate features
        assert 'processing_time_ms' in features
        assert 'whale_score' in features
        assert 'order_imbalance' in features
        assert 'trend_strength' in features
        
        # Test feature vector conversion
        feature_vector = feature_engine.get_feature_vector(features, target_size=50)
        assert feature_vector.shape == (50,)
        
        logger.info("âœ… Crypto feature engine test passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Crypto feature engine test failed: {e}")
        return False

def test_ensemble_integration():
    """Test complete ensemble integration"""
    logger.info("ðŸ§ª Testing complete ensemble integration...")
    
    try:
        from enhanced.ml.ensemble import EnhancedTradingEnsemble
        
        # Create comprehensive ensemble configuration
        config = {
            "seed": 42,
            "tcn_config": {
                "input_shape": [1, 100, 50],
                "num_channels": [64, 128, 256],
                "kernel_size": 3,
                "dropout_rate": 0.2,
                "attention_heads": 8,
                "use_attention": True,
                "market_regime_aware": True,
                "whale_detection": True,
                "feature_dims": 50
            },
            "tabnet_config": {
                "feature_dim": 50,
                "n_decision_steps": 4,
                "attention_dim": 16
            },
            "ppo_config": {
                "hidden_dims": [128, 128, 64],
                "learning_rate": 3e-4
            },
            "feature_config": {
                "short_window": 10,
                "medium_window": 50,
                "include_whale_features": True
            },
            "inference_config": {
                "target_latency_ms": 5.0,
                "enable_quantization": True
            },
            "ensemble_weights": {
                "tcn": 0.35,
                "tabnet": 0.25,
                "ppo": 0.20,
                "xgboost": 0.15,
                "lstm": 0.05
            }
        }
        
        # Create ensemble
        logger.info("Creating enhanced trading ensemble...")
        ensemble = EnhancedTradingEnsemble(config)
        
        # Test prediction
        test_features = {
            "tcn_input": jnp.ones((1, 100, 50)),
            "market_features": {
                "volume_profile": jnp.ones((1, 100, 10)),
                "order_flow": jnp.ones((1, 100, 5))
            }
        }
        
        logger.info("Testing ensemble prediction...")
        prediction = ensemble.predict(test_features)
        
        # Validate prediction structure
        assert 'signal' in prediction
        assert 'confidence' in prediction
        assert 'raw_prediction' in prediction
        assert 'performance' in prediction
        
        # Validate signal values
        assert prediction['signal'] in [-1, 0, 1]
        assert 0.0 <= prediction['confidence'] <= 1.0
        
        logger.info("âœ… Ensemble integration test passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ensemble integration test failed: {e}")
        return False

def test_performance_benchmarking():
    """Test performance benchmarking system"""
    logger.info("ðŸ§ª Testing performance benchmarking...")
    
    try:
        from enhanced.performance.comprehensive_benchmark import run_comprehensive_benchmark
        
        # Create mock ensemble for testing
        class MockEnsemble:
            def predict(self, features):
                time.sleep(0.001)  # Simulate 1ms processing
                return {
                    'signal': 1,
                    'confidence': 0.8,
                    'inference_time_ms': 1.0
                }
        
        mock_ensemble = MockEnsemble()
        
        # Run quick benchmark
        config = {
            'benchmark_iterations': 50,
            'warmup_iterations': 10,
            'memory_profiling_duration': 3,
            'accuracy_test_samples': 50,
            'save_detailed_results': False,
            'generate_report': False
        }
        
        logger.info("Running performance benchmark...")
        results = run_comprehensive_benchmark(mock_ensemble, config)
        
        # Validate results structure
        assert 'latency' in results
        assert 'throughput' in results
        assert 'memory' in results
        assert 'accuracy' in results
        assert 'overall' in results
        
        # Check overall assessment
        overall = results['overall']
        assert 'targets_met' in overall
        assert 'overall_score' in overall
        assert 'production_ready' in overall
        
        logger.info("âœ… Performance benchmarking test passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Performance benchmarking test failed: {e}")
        return False

def run_performance_validation():
    """Run actual performance validation against targets"""
    logger.info("ðŸŽ¯ Running performance validation against targets...")
    
    try:
        from enhanced.ml.ensemble import EnhancedTradingEnsemble
        from enhanced.performance.comprehensive_benchmark import run_comprehensive_benchmark
        
        # Create production-like configuration
        config = {
            "seed": 42,
            "tcn_config": {
                "input_shape": [1, 100, 50],
                "num_channels": [32, 64, 128],  # Smaller for faster testing
                "kernel_size": 3,
                "dropout_rate": 0.2,
                "use_attention": True,
                "feature_dims": 50
            },
            "tabnet_config": {
                "feature_dim": 50,
                "n_decision_steps": 3,  # Smaller for faster testing
                "attention_dim": 16
            }
        }
        
        # Create ensemble
        logger.info("Creating ensemble for performance validation...")
        ensemble = EnhancedTradingEnsemble(config)
        
        # Run performance benchmark
        benchmark_config = {
            'target_latency_ms': 5.0,
            'target_throughput': 1000,
            'target_memory_gb': 2.0,
            'target_accuracy': 0.75,
            'benchmark_iterations': 200,
            'warmup_iterations': 50,
            'memory_profiling_duration': 10,
            'accuracy_test_samples': 200,
            'save_detailed_results': True,
            'generate_report': True
        }
        
        logger.info("Running comprehensive performance validation...")
        results = run_comprehensive_benchmark(ensemble, benchmark_config)
        
        # Analyze results
        overall = results['overall']
        targets_met = overall['targets_met']
        
        logger.info("ðŸ“Š PERFORMANCE VALIDATION RESULTS:")
        logger.info(f"   Overall Score: {overall['overall_score']:.1%}")
        logger.info(f"   Targets Achieved: {overall['targets_achieved']}/{overall['total_targets']}")
        logger.info(f"   Production Ready: {overall['production_ready']}")
        
        logger.info("ðŸ“‹ TARGET BREAKDOWN:")
        for target, met in targets_met.items():
            status = 'âœ…' if met else 'âŒ'
            logger.info(f"   {target.title()}: {status}")
        
        # Detailed metrics
        if 'latency' in results:
            latency = results['latency']['latency_stats']
            logger.info(f"   P90 Latency: {latency['p90_ms']:.3f}ms (Target: <5ms)")
        
        if 'throughput' in results:
            throughput = results['throughput']['single_threaded']
            logger.info(f"   Throughput: {throughput['throughput_per_second']:.0f}/sec (Target: >1000/sec)")
        
        if 'memory' in results:
            memory = results['memory']['memory_stats']
            logger.info(f"   Peak Memory: {memory['peak_mb']:.1f}MB (Target: <2048MB)")
        
        return overall['production_ready'], results
        
    except Exception as e:
        logger.error(f"âŒ Performance validation failed: {e}")
        return False, {}

def main():
    """Run comprehensive integration tests"""
    logger.info("ðŸš€ STARTING ENHANCED TRADING ENSEMBLE INTEGRATION TESTS")
    logger.info("=" * 70)
    
    test_results = {}
    
    # Run individual component tests
    tests = [
        ("Import Test", test_imports),
        ("Enhanced TCN Test", test_enhanced_tcn),
        ("TabNet Test", test_tabnet),
        ("PPO Agent Test", test_ppo_agent),
        ("Crypto Feature Engine Test", test_crypto_feature_engine),
        ("Ensemble Integration Test", test_ensemble_integration),
        ("Performance Benchmarking Test", test_performance_benchmarking)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nðŸ§ª Running {test_name}...")
        try:
            result = test_func()
            test_results[test_name] = result
            if result:
                passed_tests += 1
                logger.info(f"âœ… {test_name} PASSED")
            else:
                logger.error(f"âŒ {test_name} FAILED")
        except Exception as e:
            logger.error(f"âŒ {test_name} FAILED with exception: {e}")
            test_results[test_name] = False
    
    # Run performance validation
    logger.info(f"\nðŸŽ¯ Running Performance Validation...")
    production_ready, perf_results = run_performance_validation()
    test_results["Performance Validation"] = production_ready
    
    if production_ready:
        passed_tests += 1
        logger.info("âœ… Performance Validation PASSED")
    else:
        logger.error("âŒ Performance Validation FAILED")
    
    total_tests += 1
    
    # Final summary
    logger.info("\n" + "=" * 70)
    logger.info("ðŸ† INTEGRATION TEST SUMMARY")
    logger.info("=" * 70)
    
    logger.info(f"Tests Passed: {passed_tests}/{total_tests}")
    logger.info(f"Success Rate: {passed_tests/total_tests:.1%}")
    
    if passed_tests == total_tests:
        logger.info("ðŸŽ‰ ALL TESTS PASSED! Enhanced Trading Ensemble is ready for production.")
        logger.info("\nâœ… VALIDATION COMPLETED:")
        logger.info("   â€¢ TCN model integrated and operational")
        logger.info("   â€¢ TabNet feature selection implemented")
        logger.info("   â€¢ PPO agent enhanced for crypto trading")
        logger.info("   â€¢ Performance benchmarks validate <5ms latency target")
        logger.info("   â€¢ Integration with existing ensemble architecture")
        logger.info("   â€¢ Progress documented in actual source files")
        logger.info("   â€¢ Performance monitoring integrated")
        
        return True
    else:
        logger.error(f"âŒ {total_tests - passed_tests} TEST(S) FAILED")
        logger.error("Please review failed tests before production deployment.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)